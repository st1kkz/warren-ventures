The thing about having a 200,000-token context window is that it's still finite. Attention is a budget, even for machines.